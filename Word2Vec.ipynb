{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpJcw8bmYrO3"
      },
      "source": [
        "# Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQxedT2cYnsU"
      },
      "source": [
        "In TF-IDF and Bag-of-words embedding models nearness of embeddings does not mean semantic nearness of embedded words. The motivation of Word2Vec models is to build embeddings in the space of a given dimension and catch words' semantic nearness. For example, vectors for words \"cat\" and \"dog\" might be nearer to each other than to \"window\" word.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://sketch.io/render/sk-7be0ce4db7bf773968b68aee3eab9a25.jpeg\" width=\"500\"/>\n",
        "</div>\n",
        "\n",
        "Technically, we can introduce embeddings of given dimension as follows. Let's take an ordered dictionary of all words, let its size be $N$. By the word of $i$ index number we build a vector wich consists of zeroes but at the $i$ place we put 1.\n",
        "Next, we multiply the vector by $N\\times M$ matrix $W_{IN}$. That's how we obtain word's representation of $M$ dimension. To get back from latent representation to initial one, we take $M\\times N$ matrix $W_{OUT}$. We multiply the latent representasion vector and apply $softmax$ to the output. As a result, we obtain a probability distribution on the dictionary that maps vectors from latent space and words from dictionary.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://i.stack.imgur.com/OpupG.png\" width=\"500\"/>\n",
        "</div>\n",
        "\n",
        "By the way, we obtained a fully connected neural network with 1 hidden layer.\n",
        "\n",
        "The only thing left is to train $W_{IN}$ and $W_{OUT}$ so that hidden representation was meaningful. The idea of learning Word2Vec models is to maximize corpora likelihood predicting context (skip-gram model):\n",
        "\n",
        "![\\arg \\max_{\\theta} \\prod_{w\\in texts}\\left[\\prod_{w' \\in context(w)} \\rm p(w' | w, \\theta)\\right]](https://latex.codecogs.com/gif.latex?%5Carg%20%5Cmax_%7B%5Ctheta%7D%20%5Cprod_%7Bw%5Cin%20texts%7D%5Cleft%5B%5Cprod_%7Bw%27%20%5Cin%20context%28w%29%7D%20%5Crm%20p%28w%27%20%7C%20w%2C%20%5Ctheta%29%5Cright%5D)\n",
        "\n",
        "or a word by its context (CBOW model):\n",
        "\n",
        "![\\arg \\max_{\\theta} \\prod_{w\\in texts} \\rm p (w | context(w), \\theta)](https://latex.codecogs.com/gif.latex?%5Carg%20%5Cmax_%7B%5Ctheta%7D%20%5Cprod_%7Bw%5Cin%20texts%7D%20%5Crm%20p%20%28w%20%7C%20context%28w%29%2C%20%5Ctheta%29)\n",
        "\n",
        "Illustration of both approaches:\n",
        "![img](https://www.researchgate.net/profile/Nailah_Al-Madi/publication/319954363/figure/fig1/AS:552189871353858@1508663732919/CBOW-and-Skip-gram-models-architecture-1.png)\n",
        "\n",
        "The next step is learning the model, it was explained in very clear manner in the original source:\n",
        "[Tomas Mikolov et al, Distributed Representations of Words and Phrases\n",
        "and their Compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf). \n",
        "\n",
        "Some interesting materials:\n",
        "* https://arxiv.org/pdf/1402.3722.pdf\n",
        "* https://medium.com/analytics-vidhya/maths-behind-word2vec-explained-38d74f32726b\n",
        "* http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
        "\n",
        "\n",
        "Let's move to practice. Here are several ways: to train our own word2vec model, to use a pre-trained model or to train starting from pre-trained wheights. Some frameworks for working with texts and applying pre-trained models:\n",
        "* [gensim](https://radimrehurek.com/gensim/)\n",
        "* [fasttext](https://fasttext.cc/)\n",
        "* [tensorflow](https://tfhub.dev/s?module-type=text-embedding&publisher=google)\n",
        "\n",
        "As an example, gensim will be used below to try a pre-trained model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qvp2eemDlpc4"
      },
      "source": [
        "## Dataset\n",
        "The problem is to classify IMDB reviews "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQhL4EpEEnJ1",
        "outputId": "b0496c15-5211-4037-a5a0-ed943bb91a7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('train.csv', index_col=0)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I think they really let the quality of the DVD...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I'm sorry but this is just awful. I have told ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The Japenese sense of pacing, editing and musi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>In the '60's/'70's, David Jason was renowned f...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"Hail The Woman\" is one of the most moving fil...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  label\n",
              "0  I think they really let the quality of the DVD...      0\n",
              "1  I'm sorry but this is just awful. I have told ...      0\n",
              "2  The Japenese sense of pacing, editing and musi...      0\n",
              "3  In the '60's/'70's, David Jason was renowned f...      1\n",
              "4  \"Hail The Woman\" is one of the most moving fil...      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnglwgThmCdl"
      },
      "source": [
        "## Data preprocessing\n",
        "\n",
        "The purpose of preprocessing is to get tokens (words) from texts which represent a dictionary. Basic steps include:\n",
        "* Tokenization \n",
        "* Filtration of non-words, stop-words and short words\n",
        "* Lemmatization (e.g., \"likes\" to \"like\")\n",
        "\n",
        "To walk through those steps we will use nltk:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xqlqS_VFg2t",
        "outputId": "ee153d5a-c524-45d0-8c7e-e9c71ce28d24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "en_stop = list(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer() \n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    tokens = [t for t in tokens if\n",
        "              re.match(r'[^\\W\\d]*$', t) and (len(t) > 2) and (t not in en_stop)]\n",
        "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "    return tokens\n",
        "\n",
        "tokens = df['review'].apply(tokenize)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2tzA9njJVZl",
        "outputId": "f46158e5-b420-4979-bebf-faf65987f099",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        [think, really, let, quality, dvd, production,...\n",
              "1        [sorry, awful, told, people, film, bad, acting...\n",
              "2        [japenese, sense, pacing, editing, musical, sc...\n",
              "3        [david, jason, renowned, many, supporting, rol...\n",
              "4        [hail, woman, one, moving, film, ever, seen, e...\n",
              "                               ...                        \n",
              "39995    [come, across, gem, movie, like, realize, grea...\n",
              "39996    [often, way, write, comment, warn, anyone, mig...\n",
              "39997    [extremely, silly, little, seen, film, slavery...\n",
              "39998    [saw, movie, scary, thing, people, talking, mo...\n",
              "39999    [though, film, seems, trying, market, horror, ...\n",
              "Name: review, Length: 40000, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBGARBl4bufH"
      },
      "source": [
        "## **Learning the Word2Vec model**\n",
        "We will use IMDB texts corpus for learning Word2Vec model. Latent space dimension is 64, window size of 3\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn8mf_-n6AvQ"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.models.phrases import Phrases, Phraser"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MSeTfO7i39Z",
        "outputId": "2e04b4ac-6ff7-420b-c7ba-e2cdd6f99594",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "bigrams = Phrases(sentences=tokens)\n",
        "trigrams = Phrases(sentences=bigrams[tokens])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFY_fUV6j8bx"
      },
      "source": [
        "bigrams = Phraser(bigrams)\n",
        "trigrams = Phraser(trigrams)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iGhI2JhQtY5"
      },
      "source": [
        "model = Word2Vec(tokens, size=300, window=6, min_count=4, iter=100, sg=0, sample=1e-5, workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7LuQztOZ-GA"
      },
      "source": [
        "## **Векторное представление текста**\n",
        "We obtained vector embeddings for single words. There are several ways represent a text. We will try the way of taking an avarage vector:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOlna3iiTkqh",
        "outputId": "b9ce05cb-c1d3-4a16-9c7a-d33853c4d366",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def encode(list_of_tokens):\n",
        "    x = np.array([model.wv[t] for t in list_of_tokens if t in model.wv.vocab])\n",
        "\n",
        "    return np.concatenate((np.mean(x, axis=0), np.median(x, axis=0)))\n",
        "\n",
        "fts = np.array([encode(t) for t in tokens])\n",
        "fts.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40000, 600)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFbNgONbVsAs"
      },
      "source": [
        "Finally, we obtained 64 features for each text. Now we can move to classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0vPmOqtVtIO"
      },
      "source": [
        "**Train-test split**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pw1hp5-IUxMv"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(fts, df.label.values,\n",
        "                                                    test_size=0.2, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmiMNAA-VkK-"
      },
      "source": [
        "**Classification model** <br>\n",
        "\n",
        "As an example, let's take logistic regression:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itHOFRSbU1mH"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression(solver='lbfgs', max_iter=3000).fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNowKt6OVgs0"
      },
      "source": [
        "Check metrics:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP-BFWDEU3lH",
        "outputId": "f439e7f4-5c56-4778-d9d3-a8172da7284c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "predicts = clf.predict(X_train)\n",
        "print('Train\\n', classification_report(y_train, predicts, digits=4))\n",
        "\n",
        "predicts = clf.predict(X_test)\n",
        "print('Test\\n', classification_report(y_test, predicts, digits=4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8945    0.8921    0.8933     16057\n",
            "           1     0.8917    0.8941    0.8929     15943\n",
            "\n",
            "    accuracy                         0.8931     32000\n",
            "   macro avg     0.8931    0.8931    0.8931     32000\n",
            "weighted avg     0.8931    0.8931    0.8931     32000\n",
            "\n",
            "Test\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8796    0.8820    0.8808      4010\n",
            "           1     0.8811    0.8787    0.8799      3990\n",
            "\n",
            "    accuracy                         0.8804      8000\n",
            "   macro avg     0.8804    0.8804    0.8804      8000\n",
            "weighted avg     0.8804    0.8804    0.8804      8000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Re2yVFP8p4pi"
      },
      "source": [
        "Let's try Support Vector Classification:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k87zFgbjIiQ8"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "clf = SVC().fit(fts, df.label.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQNGzZWNI0A1",
        "outputId": "b9884215-095c-4730-d163-2a82ebf97671",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "predicts = clf.predict(X_train)\n",
        "print('Train\\n', classification_report(y_train, predicts, digits=4))\n",
        "\n",
        "predicts = clf.predict(X_test)\n",
        "print('Test\\n', classification_report(y_test, predicts, digits=4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9219    0.9168    0.9194     16048\n",
            "           1     0.9168    0.9219    0.9193     15952\n",
            "\n",
            "    accuracy                         0.9193     32000\n",
            "   macro avg     0.9194    0.9194    0.9193     32000\n",
            "weighted avg     0.9194    0.9193    0.9193     32000\n",
            "\n",
            "Test\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8995    0.8910    0.8952      4019\n",
            "           1     0.8910    0.8995    0.8952      3981\n",
            "\n",
            "    accuracy                         0.8952      8000\n",
            "   macro avg     0.8953    0.8953    0.8952      8000\n",
            "weighted avg     0.8953    0.8952    0.8952      8000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OaD6RO7qD_K"
      },
      "source": [
        "The score is better than Logistic Regression produced"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6G17rtBdXJg4"
      },
      "source": [
        "test = pd.read_csv('test.csv', index_col=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgq4t-LnqRCY"
      },
      "source": [
        "## Pre-trained model\n",
        "We will use pre-trained Word2Vec model trained on Wikipedia articles (\"Glove-wiki-gigaword-300\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdmflkOWVDVp"
      },
      "source": [
        "tok = test['review'].apply(tokenize)\n",
        "mahmax = np.array([encode(t) for t in tok])\n",
        "predicted = clf.predict(mahmax)\n",
        "pd.DataFrame({'Predicted': predicted}).to_csv('/content/drive/My Drive/Colab Notebooks/solution.csv', index_label='Id')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DuKuBBgKeHs",
        "outputId": "9a971252-6147-4f71-e17e-6f09b44a23d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "model_pre = api.load(\"glove-wiki-gigaword-300\")  # load glove vectors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[===========================================-------] 87.7% 329.9/376.1MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:410: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIrmssNxZuD7",
        "outputId": "47ba278d-9016-4ae8-eaa4-931f1df44de9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "def encode1(list_of_tokens):\n",
        "    x = np.array([model_pre.wv[t] for t in list_of_tokens if t in model_pre.wv.vocab])\n",
        "\n",
        "    return np.concatenate((np.mean(x, axis=0), np.max(x, axis=0), np.median(x, axis=0)))\n",
        "\n",
        "fts_pre = np.array([encode1(t) for t in tokens])\n",
        "fts_pre.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40000, 900)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzxmUJ4iah_n"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(fts_pre, df.label.values,\n",
        "                                                    test_size=0.2, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LF4k0ne2se1J"
      },
      "source": [
        "Check metrics for pre-trained Word2Vec model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFFt1o0ha21d"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression(solver='lbfgs', max_iter=1500).fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVxwboOaa5cF",
        "outputId": "1b02b5ea-bb2c-44cf-d53c-ae880438ffa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "predicts = clf.predict(X_train)\n",
        "print('Train\\n', classification_report(y_train, predicts, digits=4))\n",
        "\n",
        "predicts = clf.predict(X_test)\n",
        "print('Test\\n', classification_report(y_test, predicts, digits=4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8577    0.8500    0.8538     16085\n",
            "           1     0.8498    0.8575    0.8536     15915\n",
            "\n",
            "    accuracy                         0.8537     32000\n",
            "   macro avg     0.8537    0.8537    0.8537     32000\n",
            "weighted avg     0.8538    0.8537    0.8537     32000\n",
            "\n",
            "Test\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8361    0.8478    0.8419      3982\n",
            "           1     0.8470    0.8352    0.8411      4018\n",
            "\n",
            "    accuracy                         0.8415      8000\n",
            "   macro avg     0.8416    0.8415    0.8415      8000\n",
            "weighted avg     0.8416    0.8415    0.8415      8000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InoKfu3hs0OC"
      },
      "source": [
        "In this case, pre-trained model shows worse results than our own model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6j5eBIja66F",
        "outputId": "68bd4c49-0c9c-4132-9efb-480f1b27d6c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(list(tokens)[0][:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['think', 'really', 'let', 'quality', 'dvd', 'production', 'get', 'away', 'rented', 'dvd']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}